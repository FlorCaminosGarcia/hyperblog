name: Azure AI Foundry
info: 
  icon: foundry
  platform: azure-ai-foundry
  task: "Text Generation"
features:
  streaming: true
  structuredOutputs: true
  tokenCounting: false
  isEditable: true
  tools: true
  attachments:
    - type: image
      mimeTypes:
        - image/jpeg
        - image/png
        - image/gif
        - image/webp
    - type: text
      aitkEnhanced: true
      mimeTypes:
        - application/pdf
        - application/vnd.openxmlformats-officedocument.wordprocessingml.document
        - application/epub+zip
        - application/vnd.oasis.opendocument.text
        - application/rtf
        - text/csv
        - text/plain
        - text/html
        - application/json
        - application/vnd.openxmlformats-officedocument.presentationml.presentation
        - application/vnd.oasis.opendocument.presentation
        - text/javascript
        - text/css
        - application/x-httpd-php
        - application/x-sh
        - text/markdown
  promptGenerator: system_prompt
  testDataGenerator: true
parameterSchema:
  enabled:
    - name: system_prompt
    - name: max_tokens
      default: 4096
    - name: temperature
    - name: top_p
    - name: presence_penalty
    - name: frequency_penalty
models:
  - name: AI21-Jamba-1.5-Large
    registryName: azureml-ai21
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
  - name: AI21-Jamba-1.5-Mini
    registryName: azureml-ai21
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
  - name: Codestral-2501
    registryName: azureml-mistral
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
    features:
      responseFormats:
        - text
        - json_schema
  - name: cohere-command-a
    registryName: azureml-cohere
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
        - name: frequency_penalty
        - name: presence_penalty
    features:
      responseFormats:
        - text
        - json_schema
  - name: Cohere-command-r
    registryName: azureml-cohere
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
        - name: frequency_penalty
        - name: presence_penalty
  - name: Cohere-command-r-08-2024
    registryName: azureml-cohere
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
        - name: frequency_penalty
        - name: presence_penalty
  - name: Cohere-command-r-plus
    registryName: azureml-cohere
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
        - name: frequency_penalty
        - name: presence_penalty
  - name: Cohere-command-r-plus-08-2024
    registryName: azureml-cohere
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
        - name: frequency_penalty
        - name: presence_penalty
  - name: DeepSeek-R1
    info:
      reasoning: true
    registryName: azureml-deepseek
    popular: 3
    parameterSchema:
      enabled:
        - name: max_tokens
          default: 2048
          max: 4096
    features:
      extractReasoningFromMessage: true
  - name: DeepSeek-R1-0528
    registryName: azureml-deepseek
    info:
      reasoning: true
    parameterSchema:
      enabled:
        - name: max_tokens
          default: 2048
          max: 4096
    features:
      extractReasoningFromMessage: true
  - name: DeepSeek-V3
    registryName: azureml-deepseek
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
        - name: presence_penalty
          default: 0
        - name: frequency_penalty
          default: 0
  - name: DeepSeek-V3-0324
    registryName: azureml-deepseek
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
        - name: presence_penalty
          default: 0
        - name: frequency_penalty
          default: 0
    features:
      responseFormats:
        - text
        - json_schema
  - name: gpt-35-turbo
    registryName: azure-openai
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 4096
          max: 4096
        - name: temperature
        - name: top_p
  - name: gpt-35-turbo-16k
    registryName: azure-openai
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 4096
          max: 4096
        - name: temperature
        - name: top_p
  - name: gpt-35-turbo-instruct
    registryName: azure-openai
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 4096
          max: 4096
        - name: temperature
        - name: top_p
  - name: gpt-4
    registryName: azure-openai
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 4096
          max: 4096
        - name: temperature
        - name: top_p
  - name: gpt-4-32k
    registryName: azure-openai
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 4096
          max: 4096
        - name: temperature
        - name: top_p
  - name: gpt-4.5-preview
    registryName: azure-openai
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_completion_tokens
          default: 800
          max: 16384
        - name: temperature
        - name: top_p
        - name: frequency_penalty
        - name: presence_penalty
    features:
      responseFormats:
        - text
        - json_schema
  - name: Gretel-Navigator-Tabular
    registryName: azureml-gretel
    parameterSchema:
      enabled:
        - name: temperature
        - name: top_p
  - name: grok-3
    registryName: azureml-xai
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_completion_tokens
          default: 2048
          max: 32768
        - name: temperature
        - name: top_p
        - name: frequency_penalty
        - name: presence_penalty
    features:
      responseFormats:
        - text
        - json_schema
  - name: grok-3-mini
    registryName: azureml-xai
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_completion_tokens
          default: 16000
          max: 32768
        - name: temperature
        - name: top_p
    features:
      responseFormats:
        - text
        - json_schema
  - name: jais-30b-chat
    registryName: azureml-core42
    parameterSchema:
      enabled:
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
  - name: Llama-4-Maverick-17B-128E-Instruct-FP8
    registryName: azureml-meta
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
        - name: presence_penalty
          default: 0
        - name: frequency_penalty
          default: 0
    features:
      responseFormats:
        - text
        - json_schema
  - name: Llama-4-Scout-17B-16E-Instruct
    registryName: azureml-meta
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
        - name: presence_penalty
          default: 0
        - name: frequency_penalty
          default: 0
    features:
      responseFormats:
        - text
        - json_schema
  - name: Llama-2-13b-chat
    registryName: azureml-meta
    parameterSchema:
      enabled:
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
        - name: presence_penalty
          default: 0
        - name: frequency_penalty
          default: 0
  - name: Llama-2-70b-chat
    registryName: azureml-meta
    parameterSchema:
      enabled:
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
        - name: presence_penalty
          default: 0
        - name: frequency_penalty
          default: 0
  - name: Llama-2-7b-chat
    registryName: azureml-meta
    parameterSchema:
      enabled:
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
        - name: presence_penalty
          default: 0
        - name: frequency_penalty
          default: 0
  - name: Llama-3.2-11B-Vision-Instruct
    registryName: azureml-meta
    parameterSchema:
      enabled:
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
        - name: presence_penalty
          default: 0
        - name: frequency_penalty
          default: 0
  - name: Llama-3.2-90B-Vision-Instruct
    registryName: azureml-meta
    parameterSchema:
      enabled:
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
        - name: presence_penalty
          default: 0
        - name: frequency_penalty
          default: 0
  - name: Llama-3.3-70B-Instruct
    registryName: azureml-meta
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
        - name: presence_penalty
          default: 0
        - name: frequency_penalty
          default: 0
    features:
      responseFormats:
        - text
        - json_schema
  - name: MAI-DS-R1
    registryName: azureml
    info:
      reasoning: true
    parameterSchema:
      enabled:
        - name: max_tokens
          default: 2048
          max: 4096
    features:
      extractReasoningFromMessage: true
  - name: Meta-Llama-3-70B-Instruct
    registryName: azureml-meta
    parameterSchema:
      enabled:
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
        - name: presence_penalty
          default: 0
        - name: frequency_penalty
          default: 0
  - name: Meta-Llama-3-8B-Instruct
    registryName: azureml-meta
    parameterSchema:
      enabled:
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
        - name: presence_penalty
          default: 0
        - name: frequency_penalty
          default: 0
  - name: Meta-Llama-3.1-405B-Instruct
    registryName: azureml-meta
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
        - name: presence_penalty
          default: 0
        - name: frequency_penalty
          default: 0
  - name: Meta-Llama-3.1-70B-Instruct
    registryName: azureml-meta
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
        - name: presence_penalty
          default: 0
        - name: frequency_penalty
          default: 0
  - name: Meta-Llama-3.1-8B-Instruct
    registryName: azureml-meta
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
        - name: presence_penalty
          default: 0
        - name: frequency_penalty
          default: 0
  - name: Ministral-3B
    registryName: azureml-mistral
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
  - name: Mistral-large-2407
    registryName: azureml-mistral
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
  - name: Mistral-Large-2411
    registryName: azureml-mistral
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
  - name: mistral-medium-2505
    registryName: azureml-mistral
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
    features:
      responseFormats:
        - text
        - json_schema
  - name: Mistral-Nemo
    registryName: azureml-mistral
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
  - name: Mistral-small
    registryName: azureml-mistral
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
  - name: mistral-small-2503
    registryName: azureml-mistral
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
  - name: gpt-4.1
    popular: 2
    registryName: azure-openai
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_completion_tokens
          default: 800
          max: 32768
        - name: temperature
        - name: top_p
        - name: frequency_penalty
        - name: presence_penalty
    features:
      responseFormats:
        - text
        - json_schema
  - name: gpt-4.1-mini
    registryName: azure-openai
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_completion_tokens
          default: 800
          max: 32768
        - name: temperature
        - name: top_p
        - name: frequency_penalty
        - name: presence_penalty
    features:
      responseFormats:
        - text
        - json_schema
  - name: gpt-4.1-nano
    registryName: azure-openai
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_completion_tokens
          default: 800
          max: 32768
        - name: temperature
        - name: top_p
        - name: frequency_penalty
        - name: presence_penalty
    features:
      responseFormats:
        - text
        - json_schema
  - name: gpt-4o
    registryName: azure-openai
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 4096
          max: 4096
        - name: temperature
        - name: top_p
    features:
      responseFormats:
        - text
        - json_schema
  - name: gpt-4o-mini
    registryName: azure-openai
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 4096
          max: 4096
        - name: temperature
        - name: top_p
    features:
      responseFormats:
        - text
        - json_schema
  - name: gpt-5
    popular: 1
    registryName: azure-openai
    features:
      api:
        - responses
        - chat_completions
      responseFormats:
        - text
        - json_schema
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: api
        - name: max_completion_tokens
          default: 16384
          max: 131072
        - name: reasoning_effort
        - name: reasoning_summary
          when:
            parameter: api
            value: Responses
  - name: gpt-5-chat
    registryName: azure-openai
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 16384
          max: 16384
        - name: temperature
        - name: top_p
    features:
      responseFormats:
        - text
        - json_schema
  - name: gpt-5-mini
    registryName: azure-openai
    features:
      api:
        - responses
        - chat_completions
      responseFormats:
        - text
        - json_schema
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: api
        - name: max_completion_tokens
          default: 16384
          max: 131072
        - name: reasoning_effort
        - name: reasoning_summary
          when:
            parameter: api
            value: Responses
  - name: gpt-5-nano
    registryName: azure-openai
    features:
      api:
        - responses
        - chat_completions
      responseFormats:
        - text
        - json_schema
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_completion_tokens
          default: 16384
          max: 131072
        - name: reasoning_effort
        - name: reasoning_summary
          when:
            parameter: api
            value: Responses
  - name: gpt-5-codex
    registryName: azure-openai
    features:
      api:
        - responses
      responseFormats:
        - text
        - json_schema
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: api
        - name: max_completion_tokens
          default: 16384
          max: 131072
        - name: reasoning_effort
        - name: reasoning_summary
          when:
            parameter: api
            value: Responses
  - name: gpt-5-pro
    registryName: azure-openai
    features:
      api:
        - responses
      responseFormats:
        - text
        - json_schema
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: api
        - name: max_completion_tokens
          default: 16384
          max: 131072
        - name: reasoning_effort
          default: "high"
        - name: reasoning_summary
          when:
            parameter: api
            value: Responses
  - name: codex-mini
    registryName: azure-openai
    features:
      api:
        - responses
      responseFormats:
        - text
        - json_schema
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: api
        - name: max_completion_tokens
          default: 16384
          max: 131072
        - name: reasoning_effort
        - name: reasoning_summary
          when:
            parameter: api
            value: Responses
  - name: gpt-oss-120b
    registryName: azureml-openai-oss
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_completion_tokens
          default: 2048
          max: 32768
        - name: temperature
        - name: top_p
        - name: frequency_penalty
        - name: presence_penalty
    features:
      responseFormats:
        - text
        - json_schema
  - name: o1
    info:
      reasoning: true
    registryName: azure-openai
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_completion_tokens
          default: 40000
          max: 100000
        - name: reasoning_effort
    features:
      responseFormats:
        - text
        - json_schema
  - name: o1-mini
    info:
      reasoning: true
      apiVersion: "2024-12-01-preview"
    registryName: azure-openai
    parameterSchema:
      enabled:
        - name: max_completion_tokens
          default: 4096
          max: 100000
        - name: reasoning_effort
  - name: o1-preview
    info:
      reasoning: true
    registryName: azure-openai
    parameterSchema:
      enabled:
        - name: max_completion_tokens
          default: 4096
          max: 100000
        - name: reasoning_effort
  - name: o3-pro
    info:
      reasoning: true
    registryName: azure-openai
    features:
      api:
        - responses
      responseFormats:
        - text
        - json_schema
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_completion_tokens
          default: 100000
          max: 100000
        - name: reasoning_effort
  - name: o3
    info:
      reasoning: true
    registryName: azure-openai
    features:
      api:
        - responses
        - chat_completions
      responseFormats:
        - text
        - json_schema
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: api
        - name: max_completion_tokens
          default: 100000
          max: 100000
        - name: reasoning_effort
        - name: reasoning_summary
          when:
            parameter: api
            value: Responses
  - name: o3-mini
    info:
      reasoning: true
    registryName: azure-openai
    features:
      api:
        - chat_completions
      responseFormats:
        - text
        - json_schema
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_completion_tokens
          default: 100000
          max: 100000
        - name: reasoning_effort
  - name: o4-mini
    info:
      reasoning: true
      apiVersion: "2024-12-01-preview"
    registryName: azure-openai
    features:
      api:
        - responses
        - chat_completions
      responseFormats:
        - text
        - json_schema
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: api
        - name: max_completion_tokens
          default: 100000
          max: 100000
        - name: reasoning_effort
        - name: reasoning_summary
          when:
            parameter: api
            value: Responses
  - name: Phi-3-medium-128k-instruct
    registryName: azureml
    parameterSchema:
      enabled:
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
        - name: presence_penalty
          default: 0
        - name: frequency_penalty
          default: 0
  - name: Phi-3-medium-4k-instruct
    registryName: azureml
    parameterSchema:
      enabled:
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
        - name: presence_penalty
          default: 0
        - name: frequency_penalty
          default: 0
  - name: Phi-3-mini-128k-instruct
    registryName: azureml
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
        - name: presence_penalty
          default: 0
        - name: frequency_penalty
          default: 0
  - name: Phi-3-mini-4k-instruct
    registryName: azureml
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
        - name: presence_penalty
          default: 0
        - name: frequency_penalty
          default: 0
  - name: Phi-3-small-128k-instruct
    registryName: azureml
    parameterSchema:
      enabled:
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
        - name: presence_penalty
          default: 0
        - name: frequency_penalty
          default: 0
  - name: Phi-3-small-8k-instruct
    registryName: azureml
    parameterSchema:
      enabled:
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
        - name: presence_penalty
          default: 0
        - name: frequency_penalty
          default: 0
  - name: Phi-3.5-mini-instruct
    registryName: azureml
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
        - name: presence_penalty
          default: 0
        - name: frequency_penalty
          default: 0
  - name: Phi-3.5-MoE-instruct
    registryName: azureml
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
        - name: presence_penalty
          default: 0
        - name: frequency_penalty
          default: 0
  - name: Phi-3.5-vision-instruct
    registryName: azureml
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
        - name: presence_penalty
          default: 0
        - name: frequency_penalty
          default: 0
  - name: Phi-4
    registryName: azureml
    parameterSchema:
      enabled:
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
        - name: presence_penalty
          default: 0
        - name: frequency_penalty
          default: 0
  - name: Phi-4-mini-instruct
    registryName: azureml
    parameterSchema:
      enabled:
        - name: max_tokens
          default: 2048
          max: 4096
        - name: temperature
        - name: top_p
        - name: presence_penalty
          default: 0
        - name: frequency_penalty
          default: 0
  - name: Phi-4-mini-reasoning
    info:
      reasoning: true
    registryName: azureml
    parameterSchema:
      enabled:
        - name: max_tokens
          default: 4096
          max: 32768
        - name: temperature
        - name: top_p
        - name: presence_penalty
          default: 0
        - name: frequency_penalty
          default: 0
    features:
      extractReasoningFromMessage: true
  - name: Phi-4-multimodal-instruct
    registryName: azureml
    parameterSchema:
      enabled:
        - name: max_tokens
          default: 4096
          max: 4096
  - name: Phi-4-reasoning
    info:
      reasoning: true
    registryName: azureml
    parameterSchema:
      enabled:
        - name: max_tokens
          default: 4096
          max: 32768
        - name: temperature
        - name: top_p
        - name: presence_penalty
          default: 0
        - name: frequency_penalty
          default: 0
    features:
      extractReasoningFromMessage: true
  - name: tsuzumi-7b
    registryName: azureml-nttdata
    parameterSchema:
      enabled:
        - name: system_prompt
        - name: max_tokens
          default: 4096
          max: 8192
        - name: temperature
